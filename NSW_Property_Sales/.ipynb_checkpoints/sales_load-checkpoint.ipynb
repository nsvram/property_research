{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data location\n",
    "#### https://valuation.property.nsw.gov.au/embed/propertySalesInformation\n",
    "#### http://www.valuergeneral.nsw.gov.au/__psi/weekly/20181001.zip \n",
    "#### http://www.valuergeneral.nsw.gov.au/__psi/yearly/2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "http://www.valuergeneral.nsw.gov.au/__data/assets/pdf_file/0015/216402/Current_Property_Sales_Data_File_Format_2001_to_Current.pdf\n",
    "http://www.valuergeneral.nsw.gov.au/__data/assets/pdf_file/0016/216403/Property_Sales_Data_File_-_Data_Elements_V3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import collections\n",
    "import requests\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import datetime\n",
    "import time \n",
    "import glob\n",
    "import csv\n",
    "\n",
    "\n",
    "import glob\n",
    "import csv\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import datetime\n",
    "import time \n",
    "\n",
    "#from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekly load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sales_data(file_path, folder_level=-2):\n",
    "    print(\"Loading file from - {0}/**/*.DAT\".format(file_path))\n",
    "    dat_files = [f for f in glob.iglob(\"{0}/*.DAT\".format(file_path), recursive=True)]\n",
    "    foldername = (file_path.split(\"/\")[folder_level])\n",
    "    print(\"Loading sales data for the date - \" + foldername)\n",
    "    try:\n",
    "        os.remove(\"../data/sales/A_str/{0}.csv\".format(foldername))\n",
    "        os.remove(\"../data/sales/B_str/{0}.csv\".format(foldername))\n",
    "        os.remove(\"../data/sales/C_str/{0}.csv\".format(foldername))\n",
    "        os.remove(\"../data/sales/D_str/{0}.csv\".format(foldername))\n",
    "        os.remove(\"../data/sales/Z_str/{0}.csv\".format(foldername))\n",
    "    except OSError:\n",
    "        pass\n",
    "    for i in dat_files:\n",
    "        A_str = \"\"\n",
    "        B_str = \"\"\n",
    "        C_str = \"\"\n",
    "        D_str = \"\"\n",
    "        Z_str = \"\"\n",
    "        with open(i) as f:\n",
    "            lis=[line.split() for line in f]\n",
    "            for i in lis:\n",
    "                if (\"\".join(i)).split(\";\")[0] == 'A':\n",
    "                    A_str =  A_str + \";\".join((\"\".join(i)).split(\";\")) + \"\\n\"\n",
    "                if (\"\".join(i)).split(\";\")[0] == 'B':\n",
    "                    B_str = B_str + \";\".join((\"\".join(i)).split(\";\")) + \"\\n\"\n",
    "                if (\"\".join(i)).split(\";\")[0] == 'C':\n",
    "                    C_str = C_str + \";\".join((\"\".join(i)).split(\";\")) + \"\\n\"\n",
    "                if (\"\".join(i)).split(\";\")[0] == 'D':\n",
    "                    D_str = D_str + \";\".join((\"\".join(i)).split(\";\")) + \"\\n\"\n",
    "                if (\"\".join(i)).split(\";\")[0] == 'Z':\n",
    "                    Z_str = Z_str + \";\".join((\"\".join(i)).split(\";\")) + \"\\n\"\n",
    "        with open(\"../data/sales/A_str/{0}.csv\".format(foldername), \"a\") as a_myfile:\n",
    "            a_myfile.write(A_str)\n",
    "        with open(\"../data/sales/B_str/{0}.csv\".format(foldername), \"a\") as b_myfile:\n",
    "            print(\"Foldername - \" + foldername)\n",
    "            b_myfile.write(B_str)\n",
    "        with open(\"../data/sales/C_str/{0}.csv\".format(foldername), \"a\") as c_myfile:\n",
    "            c_myfile.write(C_str)\n",
    "        with open(\"../data/sales/D_str/{0}.csv\".format(foldername), \"a\") as d_myfile:\n",
    "            d_myfile.write(D_str)\n",
    "        with open(\"../data/sales/Z_str/{0}.csv\".format(foldername), \"a\") as z_myfile:\n",
    "            z_myfile.write(Z_str)\n",
    "\n",
    "def download_extract_zip(url):\n",
    "    response = requests.get(url)\n",
    "    if (response.status_code==200):\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as thezip:\n",
    "            path2download = \"../data/temp/{0}/\".format((url.split(\"/\")[-1]).replace(\".zip\",\"\"))\n",
    "            thezip.extractall(path2download)\n",
    "            print(\"Files are downloaded successfully in {0} !!!\".format(path2download))\n",
    "            load_sales_data(path2download)\n",
    "            #return path2download\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "#download_extract_zip(url)                \n",
    "\n",
    "\n",
    "\n",
    "def property_price_sq_meter(row):\n",
    "    if row['Area_Type'] == 'M':\n",
    "        return  row['Purchase_Price'] / row['Area']\n",
    "    if row['Area_Type'] =='H':\n",
    "        return  row['Purchase_Price'] / (row['Area'] * 10000)\n",
    "\n",
    "def sale_datamart(year):\n",
    "    # Data Validation\n",
    "    col_header =['Record_Type',\n",
    "    'District_Code',\n",
    "    'Property_Id',\n",
    "    'Sale_Counter',\n",
    "    'Download_Date_Time',\n",
    "    'Property_Name',\n",
    "    'Property_Unit_Number',\n",
    "    'Property_House_Number',\n",
    "    'Property_Street_Name',\n",
    "    'Property_Locality',\n",
    "    'Property_Post_Code',\n",
    "    'Area',\n",
    "    'Area_Type',\n",
    "    'Contract_Date',\n",
    "    'Settlement_Date',\n",
    "    'Purchase_Price',\n",
    "    'Zoning',\n",
    "    'Nature_of_Property',\n",
    "    'Primary_Purpose',\n",
    "    'Strata_Lot_Number',\n",
    "    'Component_code',\n",
    "    'Sale_Code',\n",
    "    'Percentage_Interest_of_Sale',\n",
    "    'Dealing_Number',\n",
    "    'Empty']\n",
    "    \n",
    "    \n",
    "    print(\"Loading datamart .. \"  + '../data/sales/B_str/{}.csv'.format(year))\n",
    "    lst_pds = [pd.read_csv(f, \n",
    "                           sep=';', \n",
    "                           error_bad_lines=False, \n",
    "                           quoting=csv.QUOTE_NONE, \n",
    "                           header=None ,  \n",
    "                           encoding='utf-8',\n",
    "                       index_col=None\n",
    "                      ) for f in glob.glob('../data/sales/B_str/{}.csv'.format(year))]\n",
    "    df_sale = pd.concat(lst_pds)\n",
    "    df_sale.columns = col_header\n",
    "    \n",
    "    # Add Downloaded_Date from \n",
    "    df_sale['Download_Date'] = pd.to_datetime(df_sale['Download_Date_Time'].str[:8])\n",
    "    df_sale['Settlement_Date_f'] = pd.to_datetime(df_sale['Settlement_Date'],format='%Y%m%d') \n",
    "\n",
    "    #Deduplicate \n",
    "    df_sale_dedup = df_sale.drop_duplicates(subset=[i for i in df_sale.columns], keep=False)\n",
    "    \n",
    "    df_sale_dedup['Property_Price_Sq_Meter'] = df_sale_dedup.apply(property_price_sq_meter, axis=1)\n",
    "    # Build Data mart\n",
    "    cols_lst     = ['Settlement_Date_f','Property_Locality', \"Property_Post_Code\"]\n",
    "    cols_lst_key = [\"Property_Price_Sq_Meter\"]\n",
    "    \n",
    "    agg_lst      = ['mean','count', 'min', 'max', 'median','std']\n",
    "    \n",
    "    df_time_ser_price = df_sale_dedup[(df_sale_dedup.Nature_of_Property == 'R') & \n",
    "                                      (df_sale_dedup.Area_Type.notnull()) & \n",
    "                                      (df_sale_dedup.Property_Unit_Number.isnull())]\\\n",
    "                                            [cols_lst + cols_lst_key ]\\\n",
    "                                                .groupby(cols_lst)\\\n",
    "                                                .agg(agg_lst)\n",
    "    \n",
    "    df_time_ser_price =  df_time_ser_price.reset_index()\n",
    "    df_time_ser_price.columns = cols_lst + [grp + '_' + agg for grp in cols_lst_key for agg in agg_lst]\n",
    "    print(len(df_time_ser_price)) \n",
    "    # Export to csv file \n",
    "    df_time_ser_price.to_csv('../data/sales/sales_{}_data_mart.csv'.format(year.replace(\"*\",\"\")),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190114.zip\n",
      "Files are downloaded successfully in ../data/temp/20190114/ !!!\n",
      "Loading sales data for the date - 20190114\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190113.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190112.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190111.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190110.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190109.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190108.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190107.zip\n",
      "Files are downloaded successfully in ../data/temp/20190107/ !!!\n",
      "Loading sales data for the date - 20190107\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190106.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190105.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190104.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190103.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190102.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20190101.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20181231.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20181230.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20181229.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20181228.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20181227.zip\n",
      "No data found\n",
      "http://www.valuergeneral.nsw.gov.au/__psi/weekly/20181226.zip\n",
      "No data found\n",
      "10.172694444656372\n"
     ]
    }
   ],
   "source": [
    "days_to_load = 20\n",
    "ts = time.time()\n",
    "for i in range(0, days_to_load):\n",
    "    a = str(datetime.date.today() - datetime.timedelta(i)).replace(\"-\",\"\")\n",
    "    url = 'http://www.valuergeneral.nsw.gov.au/__psi/weekly/{0}.zip'.format(a)\n",
    "    print(url)\n",
    "    download_extract_zip(url)\n",
    "te = time.time()\n",
    "print(te-ts)\n",
    "#185.92472124099731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datamart .. ../data/sales/B_str/2018.csv\n",
      "76006\n"
     ]
    }
   ],
   "source": [
    "sale_datamart(\"2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.valuergeneral.nsw.gov.au/__psi/yearly/2018.zip\n",
      "Files are downloaded successfully in ../data/temp/2018/ !!!\n",
      "Loading sales data for the date - 2018\n",
      "Loading sales data for the date - 2018\n",
      "Loading sales data for the date - 2018\n"
     ]
    }
   ],
   "source": [
    "# Yearly load - time delta for [2017,2016,2015] = 491.8507146835327\n",
    "ts = time.time()\n",
    "\n",
    "for yr in range(2018,2019):\n",
    "    url = 'http://www.valuergeneral.nsw.gov.au/__psi/yearly/{0}.zip'.format(yr)\n",
    "    print(url)\n",
    "    download_extract_zip(url)\n",
    "    #\n",
    "    ## Extract weekly zip files \n",
    "    dat_files = [f for f in glob.glob(\"../data/temp/{0}/*.zip\".format(yr))]\n",
    "    for file in dat_files:\n",
    "        with zipfile.ZipFile(file) as thezip:\n",
    "            thezip.extractall(\"../data/temp/{0}/\".format(yr))\n",
    "    load_sales_data(\"../data/temp/{0}/\".format(yr),-2)\n",
    "    \n",
    "    # Load yearly data \n",
    "    # For 2015 alone - since it follows different folder struture\n",
    "    #if (yr in [2015]):\n",
    "    #    load_sales_data(\"../data/temp/{0}/\".format(yr),-2)\n",
    "    #if (yr in [2016,2017]):\n",
    "    #    load_sales_data(\"../data/temp/{0}/\".format(yr),-2)\n",
    "    else:\n",
    "        load_sales_data(\"../data/temp/{0}/{0}/*/\".format(yr),-3)\n",
    "#te = time.time()\n",
    "#print(te-ts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_sales_data(\"../data/temp/{0}\".format(yr),-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mart Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## History Load \n",
    "#for yr in range(2011,2018):\n",
    "#    print(yr)\n",
    "#    sale_datamart(str(yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temp space\n",
    "#! rm -r ../data/temp/*\n",
    "#! rm -r ../data/sales/*/'*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
